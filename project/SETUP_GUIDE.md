# ğŸ¯ Complete NeuroCore RAG Setup & Troubleshooting

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FRONTEND (Next.js)                    â”‚
â”‚              http://localhost:3000/chat                  â”‚
â”‚  â€¢ React Component: app/chat/page.tsx                   â”‚
â”‚  â€¢ User types question â†’ calls /api/query               â”‚
â”‚  â€¢ Displays LLM answer + sources + metrics              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTP POST (JSON)
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 API GATEWAY (Next.js API)                â”‚
â”‚             /app/api/query/route.ts                      â”‚
â”‚         â€¢ Receives question from frontend                â”‚
â”‚         â€¢ Proxies to Python backend                      â”‚
â”‚         â€¢ Returns formatted response                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTP POST (JSON)
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                BACKEND (FastAPI + Python)                â”‚
â”‚              http://localhost:8000/api/query             â”‚
â”‚                                                         â”‚
â”‚  RAG Pipeline:                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚  â”‚   Question   â”‚                                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚         â†“                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚ Vector Search    â”‚ â† Queries FAISS index            â”‚
â”‚  â”‚ (find relevant   â”‚                                  â”‚
â”‚  â”‚  documents)      â”‚                                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚         â†“                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚ Retrieve Top-K   â”‚ (default: 3 documents)           â”‚
â”‚  â”‚ Similar Chunks   â”‚                                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚         â†“                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚ LLM Generation   â”‚ â† FLAN-T5 model                  â”‚
â”‚  â”‚ (explain answer) â”‚ â† Enhanced prompt with            â”‚
â”‚  â”‚                  â”‚   explanation instructions       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚         â†“                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚ Format Response  â”‚ â† Add sources & confidence       â”‚
â”‚  â”‚ with Sources     â”‚                                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘                          â†‘
         â”‚                          â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚  Services:                         â”‚
    â”‚  â€¢ document_loader.py              â”‚
    â”‚  â€¢ text_splitter.py                â”‚
    â”‚  â€¢ embeddings.py                   â”‚
    â”‚  â€¢ vector_store.py                 â”‚
    â”‚  â€¢ llm.py                          â”‚
    â”‚  â€¢ prompt.py (Enhanced)            â”‚
    â”‚  â€¢ rag_pipeline.py                 â”‚
    â”‚  â€¢ main.py (FastAPI)               â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Knowledge Base:              â”‚
    â”‚  â€¢ data/documents/            â”‚
    â”‚  â€¢ data/vector_index/         â”‚
    â”‚  (FAISS embeddings)           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Prerequisites

- **Python**: 3.10+ (Get from https://www.python.org/)
- **Node.js**: 16+ (Get from https://nodejs.org/)
- **RAM**: 8GB minimum
- **Disk**: 2GB for models
- **OS**: Windows, macOS, or Linux

## âœ… Step-by-Step Setup

### 1ï¸âƒ£ Clone/Prepare Project
```bash
cd your-project-directory
```

### 2ï¸âƒ£ Install Python Dependencies
```bash
pip install -r backend/requirements.txt
```

**Expected output:**
```
Successfully installed transformers torch sentence-transformers faiss-cpu pydantic fastapi uvicorn PyPDF2
```

### 3ï¸âƒ£ Install Node Dependencies
```bash
npm install
```

**Expected output:**
```
added XXX packages
```

### 4ï¸âƒ£ Create Data Directories
```bash
# Windows
mkdir data\documents

# macOS/Linux
mkdir -p data/documents
```

### 5ï¸âƒ£ Add Documents
Copy your PDF, TXT, or Markdown files to `data/documents/`

Example:
```
data/documents/
â”œâ”€â”€ machine_learning_guide.txt (included)
â”œâ”€â”€ your_document.pdf
â””â”€â”€ another_doc.md
```

### 6ï¸âƒ£ Build Knowledge Base Index
```bash
python -m backend.app.services.test_rag
```

**Expected output:**
```
Loading documents from data/documents/...
Processing machine_learning_guide.txt...
Creating embeddings...
Building FAISS index...
Vector store created with X documents
```

### 7ï¸âƒ£ Start Backend (Terminal 1)
```bash
python -m backend.app.main
```

**Expected output:**
```
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     Application startup complete
```

### 8ï¸âƒ£ Start Frontend (Terminal 2)
```bash
npm run dev
```

**Expected output:**
```
  â–² Next.js 14.0.0
  - Local:        http://localhost:3000
  - Environments: .env.local

 âœ“ Ready in 2.3s
```

### 9ï¸âƒ£ Test the System
Open browser: **http://localhost:3000/chat**

Try asking:
```
What is machine learning?
```

Expected response:
```
Machine Learning is a subset of artificial intelligence (AI) that enables 
systems to learn and improve from experience without being explicitly programmed. 
In other words, ML systems can learn and adapt to new data without human intervention.

[Retrieved from machine_learning_guide.txt - Score: 0.89]
[Retrieved from machine_learning_guide.txt - Score: 0.85]
[Retrieved from machine_learning_guide.txt - Score: 0.82]
```

## ğŸ› Troubleshooting Guide

### âŒ Error: `ModuleNotFoundError: No module named 'transformers'`

**Cause:** Python dependencies not installed

**Solution:**
```bash
pip install -r backend/requirements.txt
```

---

### âŒ Error: `CORS error` or `Failed to get response from RAG system`

**Cause:** Backend not running or not accessible

**Solution:**
1. Make sure backend is running:
```bash
python -m backend.app.main
```

2. Check the URL in `app/api/query/route.ts` is correct (default: http://localhost:8000)

3. Verify firewall isn't blocking port 8000

---

### âŒ Error: `No documents found in data/documents`

**Cause:** Documents not added or index not built

**Solution:**
1. Add documents to `data/documents/`

2. Rebuild the index:
```bash
python -m backend.app.services.test_rag
```

3. Restart backend:
```bash
python -m backend.app.main
```

---

### âŒ First query takes 1-2 minutes

**This is normal!** 

The system is:
- Loading the LLM model (FLAN-T5) into memory (~250MB)
- Creating embeddings for documents
- Running similarity search
- Generating response

Subsequent queries are much faster (~5-10 seconds).

**To speed up first query:**
- Use GPU (CUDA): Install `torch` with GPU support
- Pre-warm by making a dummy query during setup

---

### âŒ Error: `Port 8000 already in use`

**Cause:** Another process is using port 8000

**Solution:**
```bash
# Find what's using port 8000
# Windows
netstat -ano | findstr :8000

# macOS/Linux
lsof -i :8000

# Kill the process or change backend port in main.py:
# Change: port=8000 to port=8001
```

---

### âŒ Error: `Port 3000 already in use`

**Cause:** Another Next.js app is running

**Solution:**
```bash
# Run on different port
npm run dev -- -p 3001
```

---

### âŒ Chat page shows "Error" repeatedly

**Causes & Solutions:**

1. **Backend not running**
   ```bash
   python -m backend.app.main
   ```

2. **No documents loaded**
   - Add files to `data/documents/`
   - Rebuild: `python -m backend.app.services.test_rag`

3. **Browser cache issue**
   - Press Ctrl+Shift+Delete to clear cache
   - Or use Incognito mode

4. **CORS issue**
   - Make sure backend has CORS enabled (it does by default)
   - Check that `/api/query` endpoint exists:
     ```bash
     curl http://localhost:8000/api/query -X POST -H "Content-Type: application/json" -d '{"question":"test"}'
     ```

---

### âŒ Response is irrelevant to the question

**Cause:** Wrong documents retrieved or poor embeddings

**Solution 1:** Check retrieved documents
- Look at the sources shown in the response
- Are they relevant to your question?

**Solution 2:** Improve document quality
- Use well-structured, clear documents
- Avoid images (only text is indexed)

**Solution 3:** Adjust retrieval count
- In `app/chat/page.tsx`, change `top_k`:
```typescript
body: JSON.stringify({ question: userMessage.content, top_k: 5 })  // Try 5 instead of 3
```

**Solution 4:** Use better embedding model
- In `backend/app/services/embeddings.py`, change:
```python
# From:
model_name = "sentence-transformers/all-MiniLM-L6-v2"

# To:
model_name = "sentence-transformers/all-mpnet-base-v2"  # Better quality
```

---

### âŒ LLM response is cut off or incomplete

**Cause:** Max token limit reached

**Solution:** In `backend/app/services/llm.py`:
```python
# Increase max_length:
max_length=200  # Change to 300 or 400
```

---

### âŒ Out of memory error

**Cause:** LLM model too large for available RAM

**Solution 1:** Use smaller model
```python
# In backend/app/services/llm.py:
# From:
model_name = "google/flan-t5-base"  # 250MB

# To:
model_name = "google/flan-t5-small"  # 80MB
```

**Solution 2:** Use GPU
- Install GPU PyTorch:
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

---

### âŒ Embeddings error or vector store issue

**Solution:** Rebuild from scratch
```bash
# Delete old index
rmdir /s data\vector_index  # Windows
rm -rf data/vector_index    # macOS/Linux

# Rebuild
python -m backend.app.services.test_rag
```

---

## ğŸ”§ Performance Tuning

### Make Response Faster

1. **Reduce retrieved documents:**
```typescript
top_k: 1  // Get only top document
```

2. **Use smaller LLM:**
```python
"google/flan-t5-small"  # Faster inference
```

3. **Reduce document chunks:**
```python
chunk_size=256  # Smaller chunks = faster search
```

4. **Use GPU** (if available)
```bash
# Install GPU PyTorch
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

### Make Answers Better

1. **Retrieve more documents:**
```typescript
top_k: 5  // Get top 5 documents
```

2. **Use larger LLM:**
```python
"google/flan-t5-large"  # Slower but better answers
```

3. **Use better embeddings:**
```python
"sentence-transformers/all-mpnet-base-v2"
```

4. **Improve documents:**
   - Use well-structured text
   - Clear paragraphs and sections
   - Avoid images and complex formatting

---

## ğŸ§ª Testing the Integration

### Test Backend Directly
```bash
curl -X POST "http://localhost:8000/api/query" \
  -H "Content-Type: application/json" \
  -d '{"question": "What is machine learning?", "top_k": 3}'
```

### Test Status Endpoint
```bash
curl "http://localhost:8000/api/status"
```

### View API Documentation
Open: **http://localhost:8000/docs**

---

## ğŸ“Š Monitoring & Logs

### Backend Logs
The terminal running `python -m backend.app.main` shows all API requests and errors

### Frontend Logs
- Open browser DevTools (F12)
- Check Console tab for errors
- Check Network tab to see `/api/query` requests

### Check System Status
```bash
curl http://localhost:8000/api/status | python -m json.tool
```

Example output:
```json
{
  "status": "ready",
  "documents": 5,
  "embedding_model": "all-MiniLM-L6-v2",
  "llm_model": "flan-t5-base"
}
```

---

## ğŸš€ Advanced Usage

### Using Custom Prompts
Edit `backend/app/services/prompt.py` to customize:
- Response tone (more formal, casual, technical, etc.)
- Instructions (add domain-specific guidance)
- Output format (change how sources are cited)

### Adding More Documents
1. Copy files to `data/documents/`
2. Rebuild index:
```bash
python -m backend.app.services.test_rag
```

### Using Different Embedding Models
```python
# In backend/app/services/embeddings.py:

# Fast, good quality (default)
"sentence-transformers/all-MiniLM-L6-v2"

# Better quality
"sentence-transformers/all-mpnet-base-v2"

# Best quality
"sentence-transformers/all-mpnet-base-v2"

# Multilingual
"sentence-transformers/multilingual-MiniLM-L12-v2"
```

### Using Different LLM Models
```python
# In backend/app/services/llm.py:

# Fast, good quality (default)
"google/flan-t5-base"

# Better quality, slower
"google/flan-t5-large"

# Best quality, very slow
"google/flan-t5-xl"
```

---

## âœ¨ Tips for Best Results

1. **Quality Documents**: Better documents = Better answers
2. **Clear Questions**: Be specific in your queries
3. **Right Model Size**: Balance between speed and quality
4. **Patience on First Query**: First run loads models into memory
5. **Monitor Logs**: Check logs if something seems wrong

---

## ğŸ“ Need More Help?

1. Check the Troubleshooting section above
2. Verify both backend and frontend are running
3. Check browser console for errors (F12)
4. Review backend logs (terminal output)
5. Try a fresh rebuild: Delete `data/vector_index/` and rebuild

---

**You're all set! Happy querying! ğŸ‰**
