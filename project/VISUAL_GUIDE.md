# ğŸ¨ Visual Guide - NeuroCore RAG

## Expected Behavior

### âœ… Correct Setup (What You Should See)

#### 1. Backend Running
```
$ python -m backend.app.main

INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     Application startup complete
```

#### 2. Frontend Running
```
$ npm run dev

  â–² Next.js 14.0.0
  - Local:        http://localhost:3000
  - Environments: .env.local

 âœ“ Ready in 2.3s
```

#### 3. Chat Page Loaded
```
Browser: http://localhost:3000/chat

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NeuroCore RAG Chat                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚                                                 â”‚
â”‚  [Chat messages appear here]                    â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Ask a question about your documents... â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                    [Send]                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4. Asking a Question
```
User: "What is machine learning?"

System: Processing...
       Retrieving relevant documents...
       Generating explanation...
```

#### 5. Expected Answer
```
âœ… Response received!

ANSWER:
"Machine Learning is a subset of artificial intelligence (AI) 
that enables systems to learn and improve from experience without 
being explicitly programmed. In other words, ML systems can learn 
and adapt to new data without human intervention.

Machine learning uses algorithms and statistical models to analyze 
patterns in data. These algorithms can identify relationships between 
variables that humans might not easily detect. When exposed to more 
data, machine learning models can improve their accuracy and 
performance automatically."

RETRIEVED SOURCES:
âœ“ machine_learning_guide.txt (Score: 0.89) - 2.1s
âœ“ machine_learning_guide.txt (Score: 0.85) - 2.1s  
âœ“ machine_learning_guide.txt (Score: 0.82) - 2.1s
```

---

## âŒ Common Issues (What NOT to See)

### âŒ Error 1: Backend Not Running
```
Chat Response:
"Error: Failed to get response from RAG system.

Troubleshooting:
1. Ensure backend is running: python -m backend.app.main
2. Check that data/documents/ has files
3. Try rebuilding index: python -m backend.app.services.test_rag

Error details: [CONNECT_ECONNREFUSED] 127.0.0.1:8000"

FIX: Run in Terminal 1:
$ python -m backend.app.main
```

### âŒ Error 2: No Documents in Index
```
Chat Response:
"Error: No documents found in vector store"

FIX: 
1. Add documents to data/documents/
2. Rebuild index: python -m backend.app.services.test_rag
3. Restart backend
```

### âŒ Error 3: Vector Index Not Built
```
Chat Response:
"Error: Vector store not initialized"

FIX: Build index:
$ python -m backend.app.services.test_rag
```

### âŒ Error 4: Slow First Query
```
Chat Response: Takes 60-90 seconds for first query

WHY: Loading LLM model into memory (normal!)
     Subsequent queries: 5-10 seconds

OK TO WAIT! â³
```

---

## ğŸ“‚ Directory Structure

### Before Setup
```
project/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ page.tsx
â”‚   â”œâ”€â”€ chat/
â”‚   â”‚   â””â”€â”€ page.tsx
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ query/
â”‚   â”‚       â””â”€â”€ route.ts
â”‚   â””â”€â”€ ...
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ document_loader.py
â”‚   â”‚   â”‚   â”œâ”€â”€ text_splitter.py
â”‚   â”‚   â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”‚   â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”‚   â”‚   â”œâ”€â”€ llm.py
â”‚   â”‚   â”‚   â”œâ”€â”€ prompt.py
â”‚   â”‚   â”‚   â”œâ”€â”€ rag_pipeline.py
â”‚   â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”‚   â””â”€â”€ test_rag.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/     â† ADD YOUR FILES HERE
â”‚   â””â”€â”€ vector_index/  â† Created automatically
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ QUICK_START.md
â”œâ”€â”€ SETUP_GUIDE.md
â”œâ”€â”€ start_rag.bat      â† Windows users click this
â””â”€â”€ start_rag.sh       â† macOS/Linux users run this
```

### After Setup
```
project/
â”œâ”€â”€ ... (same as above)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/
â”‚   â”‚   â”œâ”€â”€ machine_learning_guide.txt  â† Sample
â”‚   â”‚   â”œâ”€â”€ your_document.pdf           â† Your files
â”‚   â”‚   â””â”€â”€ another_doc.md              â† Your files
â”‚   â””â”€â”€ vector_index/
â”‚       â”œâ”€â”€ index.faiss                 â† FAISS embeddings
â”‚       â””â”€â”€ metadata.json               â† Document info
â””â”€â”€ ... (node_modules added)
```

---

## ğŸ”„ Data Flow Diagram

### Step 1: Document Processing (One-time)
```
Your Documents
   â†“
data/documents/
   â”œâ”€â”€ file1.pdf
   â”œâ”€â”€ file2.txt
   â””â”€â”€ file3.md
   â†“
[Document Loader - Load raw text]
   â†“
[Text Splitter - Break into 512-char chunks]
   â†“
[Embeddings - Convert text to 384-dimensional vectors]
   â†“
[Vector Store - Build FAISS index]
   â†“
data/vector_index/
   â”œâ”€â”€ index.faiss
   â””â”€â”€ metadata.json
```

### Step 2: Query Processing (Every question)
```
User Question: "What is ML?"
   â†“
Frontend: app/chat/page.tsx
   â†“
Convert to embedding vector (384 dims)
   â†“
Search FAISS index: "Find 3 most similar chunks"
   â†“
Retrieved Chunks:
â”œâ”€â”€ Chunk 1 (Score: 0.89)
â”œâ”€â”€ Chunk 2 (Score: 0.85)
â””â”€â”€ Chunk 3 (Score: 0.82)
   â†“
LLM (FLAN-T5): "Read chunks, explain what is ML"
   â†“
Return: Answer + Sources + Confidence
   â†“
Display in Chat: "Machine Learning is..."
```

---

## âš¡ Performance Timeline

### First Run (With Timestamps)
```
T+0s:    python -m backend.app.main
         â†’ FastAPI server starts
         â†’ CORS middleware configured

T+5s:    First user question received
         â†’ Vector search: 50ms
         â†’ LLM model loading: 45-60s â³
         â†’ LLM inference: 30s â³
         â†’ Response formatting: 100ms

T+95s:   Answer appears on screen âœ…

T+100s:  Second question
         â†’ Vector search: 50ms
         â†’ LLM inference: 8s (model already loaded!)
         â†’ Response formatting: 100ms

T+108s:  Answer appears âœ… (Much faster!)
```

### Subsequent Runs
```
T+0s:    python -m backend.app.main
         â†’ FastAPI server starts
         â†’ CORS middleware configured
         â†’ Loads persisted FAISS index (3-5s)
         â†’ Models already downloaded

T+10s:   Ready for queries

T+15s:   First question
         â†’ Vector search: 50ms
         â†’ LLM inference: 5-10s
         â†’ Total: ~11s âœ…

T+30s:   Second question
         â†’ Total: ~11s âœ…
```

---

## ğŸ¯ Success Checklist

Before asking questions, verify:

- [ ] Python installed (python --version)
- [ ] Node.js installed (node --version)
- [ ] Dependencies installed (pip install -r backend/requirements.txt)
- [ ] Documents in data/documents/
- [ ] Index built (python -m backend.app.services.test_rag)
- [ ] Backend running (python -m backend.app.main)
  - Should see: "Uvicorn running on http://0.0.0.0:8000"
- [ ] Frontend running (npm run dev)
  - Should see: "Local: http://localhost:3000"
- [ ] Can access http://localhost:3000/chat
- [ ] Chat page loads without errors

Once all checked: **You're ready to ask questions!**

---

## ğŸ”— Important URLs

| Component | URL | Purpose |
|-----------|-----|---------|
| Chat | http://localhost:3000/chat | Ask questions |
| Frontend | http://localhost:3000 | Home page with orbiting balls |
| Backend API | http://localhost:8000 | RAG API endpoints |
| API Docs | http://localhost:8000/docs | Swagger documentation |
| API ReDoc | http://localhost:8000/redoc | ReDoc documentation |

---

## ğŸ“± Response Format Example

### Your Question
```
User Input: "What is machine learning and why is it important?"
```

### System Processing
```
Step 1: Convert question to embedding
        "What is machine learning..." â†’ [0.12, 0.34, -0.21, ...]

Step 2: Search vector index
        Find 3 most similar document chunks

Step 3: Retrieve context
        Chunk 1: "Machine Learning is a subset of artificial intelligence..."
        Chunk 2: "Machine learning is important because..."
        Chunk 3: "ML uses algorithms and statistical models..."

Step 4: Generate explanation
        LLM reads chunks + question
        Creates detailed explanation

Step 5: Format response
        Add citations, scores, timing
```

### Chat Display
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚ Q: What is machine learning and why is it          â”‚
â”‚    important?                                       â”‚
â”‚                                                     â”‚
â”‚ A: Machine Learning is a subset of artificial      â”‚
â”‚    intelligence (AI) that enables systems to       â”‚
â”‚    learn and improve from experience without       â”‚
â”‚    being explicitly programmed. In other words,    â”‚
â”‚    ML systems can learn and adapt to new data      â”‚
â”‚    without human intervention.                      â”‚
â”‚                                                     â”‚
â”‚    Machine learning is important because it can    â”‚
â”‚    identify relationships between variables that    â”‚
â”‚    humans might not easily detect, automatically    â”‚
â”‚    improving performance as it's exposed to more    â”‚
â”‚    data.                                            â”‚
â”‚                                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  SOURCES:                                          â”‚
â”‚  âœ“ machine_learning_guide.txt (0.89) - 2.1s      â”‚
â”‚  âœ“ machine_learning_guide.txt (0.85) - 2.1s      â”‚
â”‚  âœ“ machine_learning_guide.txt (0.82) - 2.1s      â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Next Steps After Setup

1. **Explore with sample document**
   - Sample ML guide included
   - Try: "What is supervised learning?"

2. **Add your own documents**
   - Place in data/documents/
   - Rebuild index
   - Ask questions about them

3. **Customize prompts**
   - Edit backend/app/services/prompt.py
   - Change tone, add instructions
   - Restart backend to apply

4. **Explore API**
   - Visit http://localhost:8000/docs
   - Try API endpoints directly
   - Understand request/response format

5. **Performance tuning**
   - Adjust top_k for more/fewer sources
   - Try different LLM models
   - Use GPU if available

---

**Ready? Start with QUICK_START.md! ğŸ‰**
